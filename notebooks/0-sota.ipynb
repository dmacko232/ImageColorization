{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lined-airfare",
   "metadata": {},
   "source": [
    "# Image Colorization SOTA Short Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-tyler",
   "metadata": {},
   "source": [
    "The main sources of this short summary are from [Recent Approaches for Image Colorization](https://hal.archives-ouvertes.fr/hal-02965137/file/colorization.pdf), [Image Colorization: A Survey and Dataset](https://arxiv.org/pdf/2008.10774.pdf), and [Grayscale Image Colorization Methods:\n",
    "Overview and Evaluation](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9512069).\n",
    "The goal of colorization is to convert a grayscale image to color image (usually image with 3 channels). Usually, the grayscale image is considered as the luminance channel of a color image. Thus, a simple baseline is to define Y = 0.299R + 0.587G + 0.114B where Y is luminance.\n",
    "Sometimes there are different spaces than RGB used such as YUV which represents luminance and crhominance. Additionally, we can convert the RGB image to YUV.\n",
    "\n",
    "There are four main apporaches to colorization:\n",
    "- hand colorization -- coloring photos by yourself, which is a tedious process; nowadays this can be done with Photoshop but thaes a long time and LOTS of manual work; this approach is bad for many photos\n",
    "- hint-based colorization -- requires human supervision such as the user giving hints \n",
    "- example-based -- uses reference image bz user and colorezes it according to it\n",
    "- automatic colorization -- automatically recovers lost colors, hardest to do but most revarding\n",
    "We focus on the automatic one.\n",
    "\n",
    "Below are short descriptions of some papers and methods they employ. Earlier there were also classical machine learnign approaches but here we highlight only the deep learning ones. Generally, there are segmentation-like approaches (first three papers), image generation-like approaches (GANs) (4-8. paper) and even Transformers (last paper):\n",
    "1. [Deep Colorization](https://openaccess.thecvf.com/content_iccv_2015/papers/Cheng_Deep_Colorization_ICCV_2015_paper.pdf) -- first deep learning (CNN) model for this task; output layer has two neurons (channels); uses Sun dataset; does not fully use one neural network but applies post filtering\n",
    "2. [Colorful Image Colorization](https://richzhang.github.io/colorization/) -- linear stacking of 8 blocks followed where one block contains 2 or 3 CNN layers followed by ReLU and batch normalization; size of picture is decreased by striding\n",
    "3. [Deep Depth Colorization](https://ieeexplore.ieee.org/document/8306886) -- pretrained ImageNet that is kept frozen; softmax classifier\n",
    "4. [Unsupervised Diverse Colorization](https://arxiv.org/abs/1702.06674) -- conditional GANs; generator: 5 conv layers with batch normalization and ReLU; grayscale image concat with every layer of generator; discriminator is composed of four conv layers and a fully connected layer to diff between fake and real values of the image.\n",
    "5. [Tandem Adversarial Networks](https://arxiv.org/abs/1704.08834) -- two tandem networks; first predicts color scheme from the outline; the second produces imalge from the color scheme; both are based on U-Net; \n",
    "6. [ICGAN]() -- GAN; generator takes grayscale input image; inspired by full convolutional networks for semantic segmentation; down sample the input image via encoding layers and then reverse -- similar to U-Net; \n",
    "7. [Learning Diverse Image Colorization](https://arxiv.org/abs/1612.01958) -- Variational AutoEncoder and Mixture Density Network; VAE learns low-dimensional embeddings; MDN was employed to generate multiple colorizations\n",
    "8. [ChromaGAN](https://arxiv.org/pdf/1907.09837v2.pdf) -- exploits geometric, perceptual and semantic features; self-supervised; colorizes realistically; generator has two branches; disctriminator is based on PatchGAN\n",
    "9. [Colorization Transformer](https://research.google/pubs/pub50009/) -- new transformer architecture for colorization which contains inner/outer decoder + encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-vehicle",
   "metadata": {},
   "source": [
    "ChromaGAN more in depth:\n",
    "- generator has two branches:\n",
    "    - first a shared VGG16 part without last three layers that is frozen during training and uses pretrained imagenet weights\n",
    "    - then it branches into one part which predicts the class and other part; the class branch further joins into the other branch and then they together create the color channels of image (a, b channels)\n",
    "    - then the image is created by joining the Lightness image (gray scale image) together with the a, b channels and converting back to RGB\n",
    "- discriminator on the other hand works as discriminator for PatchGan -- looks at each patch of pixels and decides the realness of it!\n",
    "- it is WassersteinGAN (WGAN)\n",
    "- for loss we use mean squarer error on generated images, kullback-leibler divergence for predicted class, then wasserstein loss is used for the discriminator-generator interaction)\n",
    "- it is self-supervised in sense that VGG16 is used to classify image into class and then that is used as ground truth -- however I changed this and used ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-separate",
   "metadata": {},
   "source": [
    "There were no standard benchmarking datasets until recently when [Image Colorization: A Survey and Dataset](https://arxiv.org/pdf/2008.10774.pdf) introduced Natural-Color Dataset and this is what we use! The main reason for introducing such dataset is that many images from datasets like ImageNet are not hard ENOUGH or clear ENOUGH. So, this dataset is proposed as a good benchmark -- and many SOTA approaches fail on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
